{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOGScdQjbZuJd7rQiDhQt0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artemisyang/LS_Assignment2/blob/master/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P39DU2aDAS_L",
        "colab_type": "text"
      },
      "source": [
        "1. (a) The running time of the serial code is 471.799339056015 seconds. The running time of the parallel version of the code is  seconds, which is. times faster. The parallel code is attached below. One bottlenect is that there's no effective solution to scrape the web pages in a parallel way using PyWren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR_5G85zdRI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install awscli\n",
        "! pip install pywren\n",
        "! pip install tensorflow\n",
        "! pip install dataset\n",
        "! pip install mrjob\n",
        "\n",
        "import requests, json, numpy, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import UnivariateSpline\n",
        "import pywren\n",
        "\n",
        "import dataset\n",
        "import re\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from mrjob.job import MRJob\n",
        "import boto3\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXXB7-wGfAz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# q1.py\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "db = dataset.connect('sqlite:///books.db')\n",
        "base_url = 'http://books.toscrape.com/'\n",
        "\n",
        "def scrape_books(url_list): \n",
        "    print('Now scraping page:', url)\n",
        "    r = requests.get(url)\n",
        "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    books=html_soup.select('article.product_pod')\n",
        "    id_list = []\n",
        "    for book in books:\n",
        "        book_url = book.find('h3').find('a').get('href')\n",
        "        book_url = urljoin(url, book_url)\n",
        "        path = urlparse(book_url).path\n",
        "        book_id = path.split('/')[2]\n",
        "        id_list.append(book_id)\n",
        "    return id_list\n",
        "    \n",
        "pwex = pywren.default_executor()\n",
        "\n",
        "# Scrape the pages in the catalogue\n",
        "url = base_url\n",
        "url_list = []\n",
        "inp = input('Do you wish to re-scrape the catalogue (y/n)? ')\n",
        "while True and inp == 'y':\n",
        "    print('Now scraping page:', url)\n",
        "    url_list.append(url)\n",
        "    r = requests.get(url)\n",
        "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    next_a = html_soup.select('li.next > a')\n",
        "    if not next_a or not next_a[0].get('href'):\n",
        "        break\n",
        "    url = urljoin(url, next_a[0].get('href'))\n",
        "    \n",
        "book_list = pywren.get_all_results(pwex.map(scrape_books,url_list[1:2]))\n",
        "\n",
        "db['books'].upsert({'book_id' : book_list[0],\n",
        "                    'last_seen' : datetime.now()}, \n",
        "                   ['book_id'])\n",
        "                   \n",
        "def scrape_book(html_soup, book_id):\n",
        "    main = html_soup.find(class_='product_main')\n",
        "    book = {}\n",
        "    book['book_id'] = book_id\n",
        "    book['title'] = main.find('h1').get_text(strip=True)\n",
        "    book['price'] = main.find(class_='price_color').get_text(strip=True)\n",
        "    book['stock'] = main.find(class_='availability').get_text(strip=True)\n",
        "    book['rating'] = ' '.join(main.find(class_='star-rating') \\\n",
        "                        .get('class')).replace('star-rating', '').strip()\n",
        "    book['img'] = html_soup.find(class_='thumbnail').find('img').get('src')\n",
        "    desc = html_soup.find(id='product_description')\n",
        "    book['description'] = ''\n",
        "    if desc:\n",
        "        book['description'] = desc.find_next_sibling('p') \\\n",
        "                                  .get_text(strip=True)\n",
        "    book_product_table = html_soup.find(text='Product Information').find_next('table')\n",
        "    for row in book_product_table.find_all('tr'):\n",
        "        header = row.find('th').get_text(strip=True)\n",
        "        # Since we'll use the header as a column, clean it a bit\n",
        "        # to make sure SQLite will accept it\n",
        "        header = re.sub('[^a-zA-Z]+', '_', header)\n",
        "        value = row.find('td').get_text(strip=True)\n",
        "        book[header] = value\n",
        "        \n",
        "def scrape_book_list(book_list):\n",
        "    blist = []\n",
        "    book_id = book_list\n",
        "    book_url = base_url + 'catalogue/{}'.format(book_id)\n",
        "    print('Now scraping book:', book_url)\n",
        "    r = requests.get(book_url)\n",
        "    r.encoding = 'utf-8'\n",
        "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    blist.append(book)\n",
        "    return blist\n",
        "\n",
        "all_books = pywren.get_all_results(pwex.map(scrape_book_list,book_list))\n",
        "db['book_info'].upsert(all_books[0], ['book_id'])\n",
        " \n",
        "time_elapsed = time.time() - t0\n",
        "print(time_elapsed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak4gDaebCsOt",
        "colab_type": "text"
      },
      "source": [
        "(b) A relational database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0JpB4OR4R1S",
        "colab_type": "text"
      },
      "source": [
        "2. In this question, I saved the \"book_info\" table from the database file as a text file named \"book_info.txt\". Then I ran the python program named \"q2.py\" with this text file. The code is attached below. The output returned from this program is: \\\\\n",
        "13156\t\"the\" \\\\\n",
        "8705\t\"and\" \\\\\n",
        "7882\t\"of\" \\\\\n",
        "7088\t\"a\" \\\\\n",
        "6096\t\"to\" \\\\\n",
        "4350\t\"in\" \\\\\n",
        "3136\t\"is\" \\\\\n",
        "2513\t\"her\" \\\\\n",
        "2147\t\"that\" \\\\\n",
        "2002\t\"with\" \\\\\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3GdxvT595yJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Run this line in terminal: python q2.py 'book_info.txt'\n",
        "\n",
        "# q2.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import re\n",
        "\n",
        "WORD_RE = re.compile(r\"[\\w']+\")\n",
        "\n",
        "class MRTop10Word(MRJob):\n",
        "\n",
        "    def mapper_get_words(self, _, row):\n",
        "        #yield all of the words in the descriptions\n",
        "        data = row.split('\\t')\n",
        "        for word in WORD_RE.findall(data[7]):\n",
        "            yield (word.lower(), 1)\n",
        "\n",
        "    def combiner_count_words(self, word, counts):\n",
        "        #Sum all of the words available so far\n",
        "        yield (word, sum(counts))\n",
        "\n",
        "    def reducer_count_words(self, word, counts):\n",
        "        #Arrive at a total count for each word in the descriptions\n",
        "        yield None, (sum(counts), word)\n",
        "\n",
        "    def reducer_top10(self, word, counts):\n",
        "        #Find top 10 words\n",
        "        self.alist = []\n",
        "        self.blist = []\n",
        "        for count in counts:\n",
        "            self.alist.append(count)\n",
        "        for i in range(10):\n",
        "            self.blist.append(max(self.alist))\n",
        "            self.alist.remove(max(self.alist))\n",
        "        for i in range(10):\n",
        "            yield self.blist[i]\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_words,\n",
        "                   combiner=self.combiner_count_words,\n",
        "                   reducer=self.reducer_count_words),\n",
        "            MRStep(reducer=self.reducer_top10)\n",
        "        ]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRTop10Word.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eww5Xs75paQ",
        "colab_type": "text"
      },
      "source": [
        "3. (a) The program code is attached below, along with the code for question (b). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avAbz12qD7wO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install boto3\n",
        "\n",
        "import boto3\n",
        "import time\n",
        "\n",
        "session = boto3.Session()\n",
        "\n",
        "kinesis = session.client('kinesis')\n",
        "ec2 = session.resource('ec2')\n",
        "ec2_client = session.client('ec2')\n",
        "\n",
        "response = kinesis.create_stream(StreamName = 'a_stream',\n",
        "                                 ShardCount = 1\n",
        "                                )\n",
        "\n",
        "# Is the stream active and ready to be written to/read from? Wait until it exists before moving on:\n",
        "waiter = kinesis.get_waiter('stream_exists')\n",
        "waiter.wait(StreamName='a_stream')\n",
        "\n",
        "instances = ec2.create_instances(ImageId='ami-0915e09cc7ceee3ab',\n",
        "                                 MinCount=1,\n",
        "                                 MaxCount=2,\n",
        "                                 InstanceType='t2.micro',\n",
        "                                 KeyName='yang',\n",
        "                                 SecurityGroupIds=['sg-0d26776a5d00f67f4'],\n",
        "                                 SecurityGroups=['q3'],\n",
        "                                 IamInstanceProfile=\n",
        "                                     {'Name': 'EMR_EC2_DefaultRole'},\n",
        "                                )\n",
        "\n",
        "# Wait until EC2 instances are running before moving on\n",
        "waiter = ec2_client.get_waiter('instance_running')\n",
        "waiter.wait(InstanceIds=[instance.id for instance in instances])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMBDukN4ERgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%file producer.py\n",
        "\n",
        "import boto3\n",
        "import testdata\n",
        "import json\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "def getReferrer():\n",
        "    data = {}\n",
        "    now = datetime.datetime.now()\n",
        "    str_now = now.isoformat()\n",
        "    data['EVENT_TIME'] = str_now\n",
        "    data['TICKER'] = 'AAPL'\n",
        "    price = random.random() * 100 # Assume price is in USD\n",
        "    data['PRICE'] = round(price, 2)\n",
        "    return data\n",
        "\n",
        "kinesis = boto3.client('kinesis', region_name='us-east-1')\n",
        "\n",
        "# Continously write stock price data into Kinesis stream\n",
        "while 1 == 1:\n",
        "    kinesis.put_record(StreamName = \"a_stream\",\n",
        "                       Data = json.dumps(getReferrer()),\n",
        "                       PartitionKey = \"partitionkey\"\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBu8rtIDESds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%file consumer.py\n",
        "\n",
        "import boto3\n",
        "import time\n",
        "import json\n",
        "\n",
        "kinesis = boto3.client('kinesis', region_name='us-east-1')\n",
        "sns = boto3.client('sns',\n",
        "                   region_name='us-east-1',\n",
        "                   aws_access_key_id='ASIAVWBL5PCP4SMD4NF2',\n",
        "                   aws_secret_access_key='1BpaBCWdhjTyWPH6m6+scT8IqB3Kaoede8jG5i7h',\n",
        "                   aws_session_token='FwoGZXIvYXdzEHkaDGdkRrlDLuOC0Bb7mSLCAfCJ8kQwuf4Imd0XKMVJe5LCxq/GsnZWJwfL67OPe2mDglBNhTZ1XyVs49MpZ2yX8eZK/UlDmIPaQ+OIfdoYBN2/zDTkRLp86f4OZ1hOgIyiaBtzV2o/UgAVBPEgpOsa+Cn3epRkICPwO6LPbD57MYkOECGvzEj5s3NeQDWWHUtQ0wlKPI0AOppEFH02dmQovKdGsLVhAv2WLNmxtoy76YCGMKVgZXZGiMOqpCc2/DSeyF7TroW8E0ClKUuyJvpKxFG2KKfogfYFMi3gmzfTFKwENA1VhsWyFqzJ6pRY++PA2qTjkkVyT6wnFSJrK8lpXmB3rzoebWI=')\n",
        "\n",
        "price_arn = sns.create_topic(Name='price_alert')['TopicArn']\n",
        "print(price_arn)\n",
        "\n",
        "response = sns.subscribe(\n",
        "           TopicArn = price_arn,\n",
        "           Protocol = 'email',\n",
        "           Endpoint = 'yang20@uchicago.edu')\n",
        "                   \n",
        "shard_it = kinesis.get_shard_iterator(StreamName = \"a_stream\",\n",
        "                                     ShardId = 'shardId-000000000000',\n",
        "                                     ShardIteratorType = 'LATEST'\n",
        "                                     )[\"ShardIterator\"]\n",
        "\n",
        "i = 0\n",
        "price = 0\n",
        "while 1==1:\n",
        "    out = kinesis.get_records(ShardIterator = shard_it,\n",
        "                              Limit = 1\n",
        "                             )\n",
        "    for o in out['Records']:\n",
        "        jdat = json.loads(o['Data'])\n",
        "        price = jdat['PRICE']\n",
        "        i = i + 1\n",
        "\n",
        "    if i != 0:\n",
        "        print(jdat) #print the data stream\n",
        "    \n",
        "    if price < 5:\n",
        "        print(\"Stock price: \" + str(price))\n",
        "        message = \"Stock price is below 5\"\n",
        "        responses = sns.publish(TopicArn = price_arn,\n",
        "                                Message = message,\n",
        "                                Subject = \"Price Alert\")\n",
        "        # Delete SNS topic\n",
        "        # sns.delete_topic(TopicArn=price_arn)\n",
        "    shard_it = out['NextShardIterator']\n",
        "    time.sleep(0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFnNE52UEVaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "instance_dns = [instance.public_dns_name \n",
        "                 for instance in ec2.instances.all() \n",
        "                 if instance.state['Name'] == 'running'\n",
        "               ]\n",
        "\n",
        "code = ['producer.py', 'consumer.py']\n",
        "\n",
        "! pip install paramiko scp\n",
        "\n",
        "import paramiko\n",
        "from scp import SCPClient\n",
        "ssh_producer, ssh_consumer = paramiko.SSHClient(), paramiko.SSHClient()\n",
        "\n",
        "# Initialization of SSH tunnels takes a bit of time; otherwise get connection error on first attempt\n",
        "time.sleep(5)\n",
        "\n",
        "# Install boto3 on each EC2 instance and Copy our producer/consumer code onto producer/consumer EC2 instances\n",
        "instance = 0\n",
        "stdin, stdout, stderr = [[None, None] for i in range(3)]\n",
        "for ssh in [ssh_producer, ssh_consumer]:\n",
        "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
        "    ssh.connect(instance_dns[instance],\n",
        "                username = 'ec2-user',\n",
        "                key_filename='/Users/artemisyang/Dropbox/yang.pem')\n",
        "    \n",
        "    with SCPClient(ssh.get_transport()) as scp:\n",
        "        scp.put(code[instance])\n",
        "    \n",
        "    if instance == 0:\n",
        "        stdin[instance], stdout[instance], stderr[instance] = \\\n",
        "            ssh.exec_command(\"sudo pip install boto3 testdata\")\n",
        "    else:\n",
        "        stdin[instance], stdout[instance], stderr[instance] = \\\n",
        "            ssh.exec_command(\"sudo pip install boto3\")\n",
        "\n",
        "    instance += 1\n",
        "\n",
        "# Block until Producer has installed boto3 and testdata, then start running Producer script:\n",
        "producer_exit_status = stdout[0].channel.recv_exit_status() \n",
        "if producer_exit_status == 0:\n",
        "    ssh_producer.exec_command(\"python %s\" % code[0])\n",
        "    print(\"Producer Instance is Running producer.py\\n.........................................\")\n",
        "else:\n",
        "    print(\"Error\", producer_exit_status)\n",
        "    \n",
        "# Close ssh and show connection instructions for manual access to Consumer Instance\n",
        "ssh_consumer.close; ssh_producer.close()\n",
        "\n",
        "print(\"Connect to Consumer Instance by running: ssh -i \\\"yang.pem\\\" ec2-user@%s\" % instance_dns[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GX0DBsdEjcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Terminate EC2 Instances:\n",
        "ec2_client.terminate_instances(InstanceIds=[instance.id for instance in instances])\n",
        "\n",
        "# Confirm that EC2 instances were terminated:\n",
        "waiter = ec2_client.get_waiter('instance_terminated')\n",
        "waiter.wait(InstanceIds=[instance.id for instance in instances])\n",
        "print(\"EC2 Instances Successfully Terminated\")\n",
        "\n",
        "# Delete Kinesis Stream (if it currently exists):\n",
        "try:\n",
        "    response = kinesis.delete_stream(StreamName='a_stream')\n",
        "except kinesis.exceptions.ResourceNotFoundException:\n",
        "    pass\n",
        "\n",
        "# Confirm that Kinesis Stream was deleted:\n",
        "waiter = kinesis.get_waiter('stream_not_exists')\n",
        "waiter.wait(StreamName='a_stream')\n",
        "print(\"Kinesis Stream Successfully Deleted\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzWbo-3mEyL2",
        "colab_type": "text"
      },
      "source": [
        "(b) A screenshot of the email I received is included in the README file. I chose 5 as the alert price. For some reason I wasn't able to set up the code to delete the SNS topic, so I simply deleted the topic and subscription online from my AWS account."
      ]
    }
  ]
}